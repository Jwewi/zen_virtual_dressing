{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ae6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import sys\n",
    "import keyboard\n",
    "import json\n",
    "import ast\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle\n",
    "import ipyplot\n",
    "\n",
    "model_id = \"rasta/distilbert-base-uncased-finetuned-fashion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)\n",
    "\n",
    "def classify(text):\n",
    "    preds = classifier(text, return_all_scores=True)\n",
    "    if preds[0][0]['score']  <= preds[0][1]['score']:\n",
    "        return \"Not Fashion\"\n",
    "    else:\n",
    "        return \"Fashion\"\n",
    "    \n",
    "def attribute_extraction(txt):\n",
    "    tokenized = sent_tokenize(txt)\n",
    "\n",
    "    attributes = []\n",
    "    for i in tokenized:\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "\n",
    "    for i,w in enumerate(tagged) :\n",
    "        if w[1] in ['NN','NNS','RB'] :\n",
    "            ind =i \n",
    "            attr = w[0]\n",
    "            while tagged[ind-1][1] in ['JJ','VBN','NN','RB','VBD','EX']:\n",
    "                    attr = tagged[ind-1][0] + ' ' +  attr\n",
    "                    ind = ind - 1\n",
    "                    \n",
    "            if len(attr.split())==1 and txt.split()[0].lower()=='will':\n",
    "                attr = tagged[ind-1][0] + ' ' +  attr\n",
    "                \n",
    "            if classify(attr) == 'Fashion':\n",
    "                attributes.append(attr)\n",
    "            for a in attributes:\n",
    "                for b in attributes:\n",
    "                    if (a!=b) and (a in b):\n",
    "                        attributes.remove(a)\n",
    "                \n",
    "            for a in attributes:\n",
    "                if 'fit' in a :\n",
    "                    attributes = list(map(lambda x: x.replace(a, a.replace(' fit','')), attributes))\n",
    "                if 'match' in a :  \n",
    "                    attributes = list(map(lambda x: x.replace(a, a.replace(' match','')), attributes))                                       \n",
    "                \n",
    "    return attributes        \n",
    "\n",
    "\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "\n",
    "featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
    "\n",
    "# 10% of the total data\n",
    "size = int(len(featuresets) * 0.1)\n",
    "\n",
    "# first 10% for test_set to check the accuracy, and rest 90% after the first 10% for training\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "# get the classifer from the training set\n",
    "classifiers = nltk.NaiveBayesClassifier.train(train_set)\n",
    "# to check the accuracy - 0.67\n",
    "# print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "question_types = [\"whQuestion\",\"ynQuestion\"]\n",
    "def is_ques_using_nltk(ques):\n",
    "    question_type = classifiers.classify(dialogue_act_features(ques)) \n",
    "    return question_type in question_types\n",
    "\n",
    "\n",
    "question_pattern = [\"do i\", \"do you\", \"what\", \"who\", \"is it\", \"why\",\"would you\", \"how\",\"is there\",\n",
    "                    \"are there\", \"is it so\", \"is this true\" ,\"to know\", \"is that true\", \"are we\", \"am i\", \n",
    "                   \"question is\", \"tell me more\", \"can i\", \"can we\", \"tell me\", \"can you explain\",\n",
    "                   \"question\",\"answer\", \"questions\", \"answers\", \"ask\"]\n",
    "\n",
    "helping_verbs = [\"is\",\"am\",\"can\", \"are\", \"do\", \"does\"]\n",
    "# check with custom pipeline if still this is a question mark it as a question\n",
    "\n",
    "def is_question(question):\n",
    "    question = question.lower().strip()\n",
    "    if not is_ques_using_nltk(question):\n",
    "        is_ques = False\n",
    "        # check if any of pattern exist in sentence\n",
    "        for pattern in question_pattern:\n",
    "            is_ques  = pattern in question\n",
    "            if is_ques:\n",
    "                break\n",
    "\n",
    "        # there could be multiple sentences so divide the sentence\n",
    "        sentence_arr = question.split(\".\")\n",
    "        for sentence in sentence_arr:\n",
    "            if len(sentence.strip()):\n",
    "                # if question ends with ? or start with any helping verb\n",
    "                # word_tokenize will strip by default\n",
    "                first_word = nltk.word_tokenize(sentence)[0]\n",
    "                if sentence.endswith(\"?\") or first_word in helping_verbs:\n",
    "                    is_ques = True\n",
    "                    break\n",
    "        return is_ques    \n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "model_semantick_id = \"PriaPillai/distilbert-base-uncased-finetuned-query\"\n",
    "classifier_sem = pipeline(\"text-classification\", model=model_semantick_id)\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "verb_pattern = [ps.stem(i) for i in ['match', 'suit', 'fit', 'wear', 'pair']]\n",
    "# 'be', 'go', 'are'\n",
    "\n",
    "def semantic_check_hard_coded(txt):\n",
    "    tokenized = sent_tokenize(txt)\n",
    "    verbs = []\n",
    "    \n",
    "    for i in tokenized:\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "\n",
    "    for i,w in enumerate(tagged) :\n",
    "        if w[1] in ['VB','VBD','VBN','VBG','VBP','VBZ'] :\n",
    "            verbs.append(ps.stem(w[0]))\n",
    "    \n",
    "    for v in verbs:\n",
    "        if v in verb_pattern :\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def semantic_check(text):\n",
    "    if semantic_check_hard_coded(text):\n",
    "        return True\n",
    "    preds = classifier_sem(text, return_all_scores=True)\n",
    "    if preds[0][0]['score']  <= preds[0][1]['score']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def extraction_pipeline(query):\n",
    "    if not is_question(query):\n",
    "        message = \"I am not understanding you, please enter a question that is related to fashion\"\n",
    "        return message, []\n",
    "    elif not semantic_check(query) :\n",
    "        message = \"I am not sure to get your query can you please try again ?\"\n",
    "        return message, []\n",
    "    else:\n",
    "        return \"Working ...\",attribute_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ad2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv('image_id.csv')\n",
    "frame = frame.drop(columns=[\"Unnamed: 0\"])\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "def sample(x):\n",
    "    return data[\"Attributes\"][x]\n",
    "\n",
    "def extract_from_sample(i):\n",
    "    dic = eval(sample(i))\n",
    "    a = [dic[k]['attrs'] for k in dic.keys()]\n",
    "\n",
    "    occur = [] \n",
    "    for i,obj in enumerate(a):\n",
    "        sent =  ' '.join([d[0] for d in obj] ) + ' ' + list(dic.keys())[i]\n",
    "        occur.append(sent)\n",
    "        \n",
    "    return occur\n",
    "\n",
    "\n",
    "def extract_image(attr1,attr2,k):\n",
    "    match = []\n",
    "    a = 0\n",
    "    for i,d in enumerate(data['Attributes']):\n",
    "        l = extract_from_sample(i)\n",
    "        if (attr1 in l) and (attr2 in l):\n",
    "            match.append(list(frame[frame['id']==i]['URL'])[0])\n",
    "            a = a + 1\n",
    "            if a == k:\n",
    "                break\n",
    "    \n",
    "    if len(match)>=1:\n",
    "        ipyplot.plot_images(match, max_images=20, img_width=150, show_url=False)\n",
    "    else :\n",
    "        print(\"No image found\")\n",
    "    \n",
    "    return match\n",
    "\n",
    "from simcse import SimCSE\n",
    "\n",
    "model_SIMCSE = SimCSE(\"princeton-nlp/sup-simcse-roberta-large\")\n",
    "\n",
    "with open('index.pkl', 'rb') as f:\n",
    "    index = pickle.load(f)\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "model_SIMCSE.index = index\n",
    "items = index['sentences']\n",
    "\n",
    "def similar_items(attr):\n",
    "    similar_items=[]\n",
    "    results = model_SIMCSE.search(attr,top_k=10,threshold=0.779)\n",
    "    for i in range(len(results)):\n",
    "        similar_items.append(results[i][0])\n",
    "    return similar_items\n",
    "\n",
    "matrix = pd.read_csv('Final_co-occurence_polyvore_Adel.csv')\n",
    "\n",
    "def matrix_search_advice(attr,k):\n",
    "    match = []\n",
    "    i = 0\n",
    "    append = True\n",
    "    \n",
    "    for a in matrix['bigram'] :\n",
    "        if attr in a :\n",
    "            a = tuple(a[1:-1].replace('\\'','').split(\", \"))\n",
    "\n",
    "            if attr in a[0] :\n",
    "                wrd = a[1]\n",
    "            else:\n",
    "                wrd = a[0]\n",
    "                \n",
    "            remove = False\n",
    "            if (not wrd in match) and (not attr in wrd): \n",
    "                        match.append(wrd)\n",
    "                        i = i + 1\n",
    "            if i == k:\n",
    "                break\n",
    "                \n",
    "    return match,i\n",
    "\n",
    "def matrix_search_match(attr,k):\n",
    "    match = []\n",
    "    i = 0\n",
    "    append = True\n",
    "    \n",
    "    for a in matrix['bigram'] :\n",
    "        if attr in a :\n",
    "            a = tuple(a[1:-1].replace('\\'','').split(\", \"))\n",
    "\n",
    "            if attr in a[0] :\n",
    "                wrd = a[1]\n",
    "            else:\n",
    "                wrd = a[0]\n",
    "                \n",
    "            remove = False\n",
    "            if (not wrd in match) and (not attr in wrd): \n",
    "                    for el in match : \n",
    "                        if model_SIMCSE.similarity(el,wrd) > 0.7:\n",
    "                            remove = True\n",
    "                    if not remove:\n",
    "                        match.append(wrd)\n",
    "                        i = i + 1\n",
    "            #if i == k+3:\n",
    "            if i == k:\n",
    "                break\n",
    "                \n",
    "#     for el1 in match:\n",
    "#         for el2 in match:\n",
    "#             if el1 != el2 :\n",
    "#                 if model_SIMCSE.similarity(el1,el2) > 0.7:\n",
    "#                     match.remove(el2)\n",
    "    return match,i\n",
    "\n",
    "def garment_matching(attr,k):           # Returns k best matches to the given attribute\n",
    "    \n",
    "    attr = \" \".join([lemmatizer.lemmatize(i) for i in attr.split()])\n",
    "    i= 0\n",
    "    match = []\n",
    "\n",
    "    if attr in items:\n",
    "        if k == 5 :\n",
    "            match,i = matrix_search_match(attr,k)\n",
    "        if k == 10 :\n",
    "            match,i = matrix_search_advice(attr,k)\n",
    "\n",
    "    else :\n",
    "        similar = similar_items(attr)\n",
    "        stop = False\n",
    "        ind = 0\n",
    "        while (not stop) and (ind < len(similar)):\n",
    "            print(len(similar))\n",
    "            if similar[ind] in items:\n",
    "                if k == 5:\n",
    "                    match,i = matrix_search_match(similar[ind],k)\n",
    "                if k == 10:\n",
    "                    match,i = matrix_search_advice(similar[ind],k)\n",
    "                if (i>0):\n",
    "                    stop = True \n",
    "                    attr = similar[ind]\n",
    "            ind = ind + 1\n",
    "\n",
    "    if (i==0):\n",
    "        message = 'This attribute was not found for the garment matching try another attribute!'\n",
    "        return message,[]\n",
    "        \n",
    "    return attr,match\n",
    "\n",
    "def garment_advice(attr1 , attr2, k=10):\n",
    "    match = []\n",
    "    \n",
    "    i = 0\n",
    "    attr1, match = garment_matching(attr1,k)\n",
    "    \n",
    "    #attr2 = \" \".join([lemmatizer.lemmatize(i) for i in attr2.split()])\n",
    "    \n",
    "    if match is None :\n",
    "        return attr1,None, False\n",
    "    \n",
    "    if attr2 in match:\n",
    "        return attr1,attr2,True\n",
    "    else:\n",
    "        for el in match:\n",
    "            if model_SIMCSE.similarity(el,attr2) > 0.9 :\n",
    "                return attr1,el,True\n",
    "    \n",
    "    return None, None, False\n",
    "\n",
    "def check_image(num, attr1, attr2):\n",
    "    \n",
    "    bound = eval(data['boudaries(X,y,Width,Height)'][num])\n",
    "    if (attr1 in bound.keys()) and (attr2 in bound.keys()):\n",
    "        x1,y1,x2,y2 = bound[attr1]\n",
    "        a1,b1,a2,b2 = bound[attr2]\n",
    "        \n",
    "        percentage1 = ( ((x2-x1)/6) + ((y2-y1)/6) ) / 2\n",
    "        percentage2 = ( ((a2-a1)/6) + ((b2-b1)/6) ) / 2\n",
    "        \n",
    "        center1 = np.array([ x1 + (x2-x1)/2 , y1 + (y2-y1)/2])\n",
    "        center2 = np.array([ a1 + (a2-a1)/2 , b1 + (b2-b1)/2])\n",
    "        \n",
    "        dist = np.linalg.norm(center1 - center2)\n",
    "        \n",
    "#         print(percentage1, percentage2, dist)\n",
    "#         print(center1, center2)\n",
    "        \n",
    "        if percentage1 < 20 or percentage2 < 20:\n",
    "            return False\n",
    "        else :\n",
    "            return True\n",
    "    else:\n",
    "        #print(\"One of the attributes is not found in the image\")\n",
    "        return False\n",
    "    \n",
    "def new_extract_image(attr1,attr2, k):\n",
    "    match = []\n",
    "    a = 0\n",
    "    for i,d in enumerate(data['Attributes']):\n",
    "        l = extract_from_sample(i)\n",
    "        if (attr1 in l) and (attr2 in l) and check_image(i, attr1, attr2):\n",
    "            match.append(list(frame[frame['id']==i]['URL'])[0])\n",
    "            a = a + 1\n",
    "            if a == k:\n",
    "                break\n",
    "    #else :\n",
    "        #print(\"No image found\")\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125c527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2022 19:02:30 - INFO - apscheduler.scheduler -   Scheduler started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot started....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full import config as keys\n",
    "#import config as keys\n",
    "from telegram.ext import *\n",
    "\n",
    "\n",
    "print(\"Bot started....\")\n",
    "\n",
    "def start_command(update, context): #Defines what happens when bot is started\n",
    "    update.message.reply_text('Welcome to fashion advisor \\n Please enter your query.')\n",
    "\n",
    "def end_to_end(update, context):\n",
    "    \n",
    "    query = update.message.text\n",
    "    msg, attr = extraction_pipeline(query)\n",
    "    \n",
    "    \n",
    "    if attr is None:\n",
    "        update.message.reply_text(\"An unknown problem occured please contact the support.\")\n",
    "    \n",
    "    URL = []\n",
    "    \n",
    "    if len(attr) == 1 :         # garment matching\n",
    "        attr0,match = garment_matching(attr[0],5)\n",
    "        update.message.reply_text(attr[0].capitalize() +' will match with the following attributes: ')\n",
    "        update.message.reply_text(match)\n",
    "        update.message.reply_text('\\nHere are some images of your item with some good matches:\\n')\n",
    "\n",
    "        for item in match:\n",
    "            URL = URL + new_extract_image(attr0, item,1)\n",
    "            URL = list(dict.fromkeys(URL))\n",
    "\n",
    "        chat_id = update.message.chat_id  \n",
    "\n",
    "        for i in range(len(URL)):            \n",
    "            context.bot.sendPhoto(chat_id=chat_id, photo=URL[i])\n",
    "\n",
    "\n",
    "    elif len(attr) == 2 :         #garment advice\n",
    "        attr1,attr2,g = garment_advice(attr[0] , attr[1], 10)\n",
    "        if g == True:\n",
    "            update.message.reply_text(attr[0].capitalize()+ ' would be a good match with '+ attr[1])\n",
    "            update.message.reply_text('\\nHere are some images of that combo: ')\n",
    "            URL = new_extract_image(attr1, attr2,5)\n",
    "            URL = list(dict.fromkeys(URL))\n",
    "                   \n",
    "            chat_id = update.message.chat_id  \n",
    "        \n",
    "            for i in range(len(URL)):            \n",
    "                context.bot.sendPhoto(chat_id=chat_id, photo=URL[i])            \n",
    "\n",
    "                \n",
    "        elif attr1 is None:\n",
    "            update.message.reply_text(\"Those items are not commonly worn together !\")\n",
    "        \n",
    "        else:\n",
    "            update.message.reply_text(attr1)\n",
    "            \n",
    "            \n",
    "    elif len(attr) == 0 :\n",
    "        update.message.reply_text(msg)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        update.message.reply_text('More than 2 attributes were detected, this version only support 1 attribute for garment matching and 2 for garment advice')\n",
    "\n",
    "        \n",
    "        \n",
    "def help_command(update, context): #To give specific instructions to user\n",
    "    update.message.reply_text('This bot can give you advice about what to wear with a cloath you give him (garment advice) or can tell you if 2 items are a good fit together (garment matching)  ')\n",
    "    update.message.reply_text('Here are 2 exemples of the queries: \\n  - garment advice: What can I wear with a blue pant ? \\n  - garment matching: Can I wear a blue pant with a white shirt ? ')\n",
    "    \n",
    "def error(update, context):\n",
    "    print(f\"Update {update} caused error {context.error}\")\n",
    "\n",
    "def main():\n",
    "    updater = Updater(keys.API_KEY, use_context=True)\n",
    "    dp = updater.dispatcher\n",
    "\n",
    "    dp.add_handler(CommandHandler(\"start\", start_command))\n",
    "    dp.add_handler(CommandHandler(\"help\", help_command))\n",
    "    dp.add_handler(MessageHandler(Filters.text, end_to_end))\n",
    "    dp.add_error_handler(error)\n",
    "\n",
    "    updater.start_polling()\n",
    "    updater.idle()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c52d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
